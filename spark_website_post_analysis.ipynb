{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autopep8\n",
      "  Using cached autopep8-1.7.0-py2.py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: toml in /opt/conda/envs/data3/lib/python3.9/site-packages (from autopep8) (0.10.2)\n",
      "Collecting pycodestyle>=2.9.1\n",
      "  Using cached pycodestyle-2.9.1-py2.py3-none-any.whl (41 kB)\n",
      "Installing collected packages: pycodestyle, autopep8\n",
      "Successfully installed autopep8-1.7.0 pycodestyle-2.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install autopep8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Spark Miniproject\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Stack Overflow is a collaboratively edited question-and-answer site originally focused on programming topics. Because of the variety of features tracked, including a variety of feedback metrics, it allows for some open-ended analysis of user behavior on the site.\n",
    "\n",
    "Stack Exchange (the parent organization) provides an anonymized [data dump](https://archive.org/details/stackexchange), and we'll use Spark to perform data manipulation, analysis, and machine learning on this data set. As a side note, there's also an online data explorer which allows you to query the data interactively.\n",
    "\n",
    "*Consider*: Do we need to use Spark to work with this data set? What are our alternatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Most questions can be done locally, however in some cases you may want to use cloud services. See the appropriate lecture notebooks for information on how to use cloud services.\n",
    "\n",
    "Python example:\n",
    "\n",
    "1. Edit source code in your `main.py` file, classes in a separate `classes.py` (class definitions need to be written in a separate file and then included at runtime).\n",
    "1. Run locally on a chunk using eg. `$SPARK_HOME/bin/spark-submit --py-files src/classes.py src/main.py data/stats results/stats/`\n",
    "1. Run on GCP once your testing and development are done.\n",
    "\n",
    "General tips:\n",
    "* Try `cat output_dir/* | sort -n -t , -k 1.2 -o sorted_output` to concatenate your output files, which will also be in `part-xxxxx` format.\n",
    "* You can access an interactive PySpark shell with `$SPARK_HOME/bin/pyspark`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Accessing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The data is available on S3 (`s3://dataincubator-course/spark-stack-data`). There are three sub-folders, `allUsers`, `allPosts`, and `allVotes`, which contain Gzipped XML with the following format:\n",
    "\n",
    "``` html\n",
    "<row Body=\"&lt;p&gt;I always validate my web pages, and I recommend you do the same BUT many large company websites DO NOT and cannot validate because the importance of the website looking exactly the same on all systems requires rules to be broken. &lt;/p&gt;&#10;&#10;&lt;p&gt;In general, valid websites help your page look good even on odd configurations (like cell phones) so you should always at least try to make it validate.&lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2008-10-12T20:26:29.397\" Id=\"195995\" LastActivityDate=\"2008-10-12T20:26:29.397\" OwnerDisplayName=\"Eric Wendelin\" OwnerUserId=\"25066\" ParentId=\"195973\" PostTypeId=\"2\" Score=\"0\" />\n",
    "```\n",
    "\n",
    "Data from the much smaller `stats.stackexchange.com` is available in the same format on S3 (`s3://dataincubator-course/spark-stats-data`). This site, Cross-Validated, will be used below in some instances to avoid working with the full data set for every question.\n",
    "\n",
    "The full schema is available as a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# !aws s3 cp s3://dataincubator-course/spark-stats-data/stack_exchange_schema.txt ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "You can either get the data by running the appropriate S3 commands in the terminal, or by running this block for the smaller stats data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# !mkdir -p spark-stats-data\n",
    "# !aws s3 sync --exclude '*' --include 'all*' s3://dataincubator-course/spark-stats-data/ ./spark-stats-data\n",
    "# !aws s3 sync --exclude '*' --include 'posts*zip' s3://dataincubator-course/spark-stats-data/ ./spark-stats-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "And to get the much larger full data set (be warned, this can take 20 or more minutes, so you may want to run it in the terminal to avoid locking up the notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# !mkdir -p spark-stack-data\n",
    "# !aws s3 sync --exclude '*' --include 'all*' s3://dataincubator-course/spark-stack-data/ ./spark-stack-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Data input and parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Some rows are split across multiple lines; these can be discarded. Incorrectly formatted XML can also be ignored. It is enough to simply skip problematic rows, the loss of data will not significantly impact our results on this large data sets.\n",
    "\n",
    "You will need to handle XML parsing yourself.  Our solution uses `lxml.etree` in Python.\n",
    "\n",
    "To make your code more flexible, it's also recommended to incorporate command-line arguments that specify the location of the input data and where output should be written.\n",
    "\n",
    "The goal should be to have a parsing function that can be applied to the input data to access any XML element desired. It is suggested to use a class structure so that you can create RDDs of Posts, Votes, Users, etc.\n",
    "\n",
    "``` python\n",
    "# Command line arguments using sysv or argparse in Python\n",
    "if __name__ == '__main__':\n",
    "    main(ARGS.input_dir, ARGS.output_dir)\n",
    "```\n",
    "https://towardsdatascience.com/a-simple-guide-to-command-line-arguments-with-argparse-6824c30ab1c3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 1: Bad XML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "A simple question to test your parsing code. Create an RDD of Post objects where each Post is a valid row of XML from the Cross-Validated (stats.stackexchange.com) `allPosts` data set.\n",
    "\n",
    "We are going to take several shortcuts to speed up and simplify our computations.  First, your parsing function should only attempt to parse rows that start with `<row` as these denote actual data entries. This should be done in Spark as the data is being read in from disk, without any pre-Spark processing. \n",
    "\n",
    "Return the total number of XML rows that started with `<row` that were subsequently **rejected** during your processing.  Note that the text is unicode, and contains non-ASCII characters.  You may need to re-encode to UTF-8 (depending on your XML parser)\n",
    "\n",
    "Note that this cleaned data set will be used for all subsequent questions.\n",
    "\n",
    "*Question*: Can you figure out what filters you need to put in place to avoid throwing parsing errors entirely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=temp, master=local[*]) created by __init__ at /tmp/ipykernel_53/1279373348.py:8 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53/170182534.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mElementTree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"temp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    343\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=temp, master=local[*]) created by __init__ at /tmp/ipykernel_53/1279373348.py:8 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "sc = SparkContext(\"local[*]\", \"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    try:\n",
    "        parsed_line = ET.fromstring(line)\n",
    "        return parsed_line\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def filter_xml_lines(input_location):\n",
    "    lines = sc.textFile(input_location)\n",
    "    filtered_lines = lines.filter(lambda x: '<row' in x)\n",
    "    return filtered_lines\n",
    "\n",
    "\n",
    "def parse_xml(lines):\n",
    "    return lines.map(parse_line).filter(lambda x: x != 0)\n",
    "\n",
    "\n",
    "def save_parsed_lines(parsed_lines, output_location):\n",
    "    try:\n",
    "        parsed_lines.saveAsTextFile(output_location)\n",
    "    except:\n",
    "        raise EEXIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_parsing = filter_xml_lines('./spark-stats-data/allPosts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_parsing = parse_xml(before_parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_parsed_lines(after_parsing, \"Q_1_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'before_parsing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bad_xml \u001b[38;5;241m=\u001b[39m \u001b[43mbefore_parsing\u001b[49m\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m-\u001b[39m after_parsing\u001b[38;5;241m.\u001b[39mcount()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'before_parsing' is not defined"
     ]
    }
   ],
   "source": [
    "bad_xml = before_parsing.count() - after_parsing.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 2: Favorites and scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We're interested in looking for useful patterns in the data.  If we look at the Post data again (the smaller set, `stats.stackexchange.com`), we see that many things about each post are recorded.  We're going to start by looking to see if there is a relationship between the number of times a post was favorited (the `FavoriteCount`) and the `Score`.  The score is the number of times the post was upvoted minus the number of times it was downvoted, so it is a measure of how much a post was liked.  We'd expect posts with a higher number of favorites to have better scores, since they're both measurements of how good the post is.\n",
    "\n",
    "Let's aggregate posts by the number of favorites, and find the average score for each number of favorites.  Do this for the lowest 50 numbers of favorites.\n",
    "\n",
    "If any field in the Posts or Users is missing, such as the `FavoriteCount`, you should assume it is zero. Make this assumption for all questions going forward.\n",
    "\n",
    "_Note:_ Before submitting, take a look at the numbers.  Do they follow the trend you expect?\n",
    "\n",
    "**Checkpoints**\n",
    "\n",
    "- Total score across all posts: 299469\n",
    "- Mean of first 50 favorite counts (averaging the keys themselves): 24.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fav_score_filter(parsed_line):\n",
    "    if \"Score\" in parsed_line.attrib:\n",
    "        score = parsed_line.attrib['Score']\n",
    "    else:\n",
    "        score = 0\n",
    "    if \"FavoriteCount\" in parsed_line.attrib:\n",
    "        fav_count = parsed_line.attrib['FavoriteCount']\n",
    "    else:\n",
    "        fav_count = 0\n",
    "\n",
    "    return (int(fav_count), int(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_fav = after_parsing.map(fav_score_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (0, 5), (0, 0), (0, 7), (0, 1)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_fav.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = score_fav.sortByKey(lambda x: x[0])\\\n",
    "    .reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "count = score_fav.map(lambda x: x[0]).map(lambda x: (x, 1))\\\n",
    "    .sortByKey(lambda x: x[0])\\\n",
    "    .reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sorted(list(total.join(count).map(lambda x: (x[0],x[1][0]/x[1][1])).collect()))[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m favorite_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[43mtotal\u001b[49m\u001b[38;5;241m.\u001b[39mjoin(count)\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mx[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39mcollect()))[:\u001b[38;5;241m50\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'total' is not defined"
     ]
    }
   ],
   "source": [
    "favorite_score = sorted(list(total.join(count).map(\n",
    "    lambda x: (x[0], x[1][0]/x[1][1])).collect()))[:50]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 3: Answer percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Investigate the correlation between a user's reputation and the kind of posts they make. For the 99 users with the highest reputation, single out posts which are either questions or answers and look at the percentage of these posts that are answers: *(answers / (answers + questions))*. \n",
    "\n",
    "Return a tuple of their **user ID** and this fraction.\n",
    "\n",
    "You should also return (-1, fraction) to represent the case where you average over all users (so you will return 100 entries total).\n",
    "\n",
    "Again, you only need to run this on the statistics overflow set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "* Total questions: 52,060\n",
    "* Total answers: 55,304\n",
    "* Top 99 users' average reputation: 11893.464646464647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_data = sc.textFile('./spark-stats-data/allUsers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100425"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_user_data(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            root = ET.fromstring(line)\n",
    "            if \"Id\" in root.attrib:\n",
    "                user_id = root.attrib['Id']\n",
    "                if 'Reputation' in root.attrib:\n",
    "                    reputation = root.attrib['Reputation']\n",
    "                else:\n",
    "                    reputation = 0\n",
    "                return (int(user_id), int(reputation))\n",
    "            else:\n",
    "                return ('empty')\n",
    "\n",
    "        except:\n",
    "            return ('empty')\n",
    "    else:\n",
    "        return ('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_users_data = users_data.map(filter_user_data).filter(\n",
    "    lambda x: x != 'empty').sortBy(lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50320"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_users_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_posts_with_id_and_type(parsed_line):\n",
    "    try:\n",
    "        if \"PostTypeId\" and \"OwnerUserId\" in parsed_line.attrib:\n",
    "            user_id = parsed_line.attrib['OwnerUserId']\n",
    "            ppst_type = int(parsed_line.attrib['PostTypeId'])\n",
    "            if ppst_type == 1 or ppst_type == 2:\n",
    "                return (int(user_id), ppst_type)\n",
    "            else:\n",
    "                return ('empty')\n",
    "        else:\n",
    "            return ('empty')\n",
    "    except:\n",
    "        return ('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_and_post_type = after_parsing.map(\n",
    "    filter_posts_with_id_and_type).filter(lambda x: x != 'empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = user_id_and_post_type.filter(\n",
    "    lambda x: x[1] == 1).reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "answers = user_id_and_post_type.filter(lambda x: x[1] == 2).map(\n",
    "    lambda x: (x[0], 1)).reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_peocessed_rdd = valid_users_data.leftOuterJoin(answers).leftOuterJoin(questions).map(lambda x: (\n",
    "    x[0], x[1][0][0], x[1][0][1] if x[1][0][1] else 0, x[1][1] if x[1][1] else 0)).sortBy(lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_final = q3_peocessed_rdd.map(lambda x: (\n",
    "    x[0], x[1], x[2], x[3], x[2]/(x[2]+x[3] if x[2] or x[3] else 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "total, n_count = valid_users_data.leftOuterJoin(answers).leftOuterJoin(questions)\\\n",
    "    .map(lambda x: (x[0], x[1][0][0], x[1][0][1] if x[1][0][1] else 0, x[1][1] if x[1][1] else 0))\\\n",
    "    .map(lambda x: (x[0], x[2]/(x[2]+x[3]) if x[2] or x[3] else 0, 1))\\\n",
    "    .map(lambda x: (x[1], x[2]))\\\n",
    "    .reduce(lambda x, y: (x[0]+y[0], x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_percentage = q3_final.map(lambda x: (x[0], x[4])).take(99)\n",
    "answer_percentage.append((-1, total/n_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer_percentage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m answer_percentage \u001b[38;5;241m=\u001b[39m \u001b[43manswer_percentage\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'answer_percentage' is not defined"
     ]
    }
   ],
   "source": [
    "answer_percentage = answer_percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 4: First question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We'd expect the first **question** a user asks to be indicative of their future behavior.  We'll dig more into that in the next problem, but for now let's see the relationship between reputation and how long it took each person to ask their first question.\n",
    "\n",
    "For each user that asked a question, find the difference between when their account was created (`CreationDate` for the User) and when they asked their first question (`CreationDate` for their first question).  Return this time difference in days (round down, so 2.7 days counts as 2 days) for the 100 users with the highest reputation, in the form\n",
    "\n",
    "`(UserId, Days)`\n",
    "\n",
    "**Checkpoints**\n",
    "- Users that asked a question: 23134\n",
    "- Average number of days (round each user's days, then average): 30.1074258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import timedelta, datetime\n",
    "\n",
    "# from pyspark import SparkContext\n",
    "# # import xml.etree.ElementTree as ET   \n",
    "# import os\n",
    "# from lxml import etree as Et\n",
    "\n",
    "# sc = SparkContext(\"local[*]\", \"temp\")\n",
    "\n",
    "# from pyspark.sql import SQLContext\n",
    "\n",
    "# sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_user_creation_date_data(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            root = ET.fromstring(line)\n",
    "            if \"Id\" in root.attrib:\n",
    "                user_id = root.attrib['Id']\n",
    "                if 'Reputation' in root.attrib:\n",
    "                    reputation = root.attrib['Reputation']\n",
    "                else:\n",
    "                    reputation = 0\n",
    "                if 'CreationDate' in root.attrib:\n",
    "                    creation_date = datetime.strptime(\n",
    "                        root.attrib['CreationDate'], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "                else:\n",
    "                    creation_date = None\n",
    "                return (int(user_id), (int(reputation), creation_date))\n",
    "            else:\n",
    "                return ('empty')\n",
    "\n",
    "        except:\n",
    "            return ('empty')\n",
    "    else:\n",
    "        return ('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_data = sc.textFile('./spark-stats-data/allUsers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_data_with_creation_date = users_data.map(\n",
    "    filter_user_creation_date_data).filter(lambda x: x != 'empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(70185, (1, datetime.datetime(2015, 3, 2, 18, 42, 20, 510000))),\n",
       " (70186, (6, datetime.datetime(2015, 3, 2, 19, 4, 13, 380000))),\n",
       " (70187, (1, datetime.datetime(2015, 3, 2, 19, 40, 16, 420000))),\n",
       " (70188, (1, datetime.datetime(2015, 3, 2, 19, 46, 45, 400000))),\n",
       " (70189, (101, datetime.datetime(2015, 3, 2, 19, 56, 37, 233000)))]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_data_with_creation_date.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_first_question(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            parsed_line = ET.fromstring(line)\n",
    "            if \"PostTypeId\" and \"OwnerUserId\" and 'CreationDate' in parsed_line.attrib:\n",
    "                user_id = parsed_line.attrib['OwnerUserId']\n",
    "                post_type = int(parsed_line.attrib['PostTypeId'])\n",
    "                creation_date = datetime.strptime(\n",
    "                    parsed_line.attrib['CreationDate'], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "                if post_type == 1:\n",
    "                    return (int(user_id), creation_date)\n",
    "                else:\n",
    "                    return ('empty')\n",
    "            else:\n",
    "                return ('empty')\n",
    "        except:\n",
    "            return ('empty')\n",
    "    else:\n",
    "        return ('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_post_info = before_parsing.map(post_first_question).filter(\n",
    "    lambda x: x != 'empty').reduceByKey(lambda x, y: min(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_delta(after, before):\n",
    "    if before is not None and after is not None:\n",
    "        diff = (after - before).days\n",
    "        return diff if diff > 0 else 0\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_question = used_data_with_creation_date.join(first_post_info)\\\n",
    "    .map(lambda x: (x[0], x[1][0][0], x[1][0][1], x[1][1]))\\\n",
    "    .map(lambda x: (x[0], x[1], time_delta(x[3], x[2])))\\\n",
    "    .sortBy(lambda x: -x[1])\\\n",
    "    .map(lambda x: (x[0], x[2]))\\\n",
    "    .take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rows(line):\n",
    "    if '<row' not in line:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        root = ET.fromstring(line)\n",
    "        if root.tag == 'row':\n",
    "            return dict(root.attrib)\n",
    "    except ET.ParseError:\n",
    "        pass\n",
    "    \n",
    "    return None       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_data = sc.textFile('./spark-stats-data/allUsers')\n",
    "df = users_data.map(filter_rows).filter(lambda x: x!= None).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------------+---------+-----+--------------------+--------------------+----------+-------+-----+\n",
      "|AccountId|        CreationDate|      DisplayName|DownVotes|   Id|      LastAccessDate|     ProfileImageUrl|Reputation|UpVotes|Views|\n",
      "+---------+--------------------+-----------------+---------+-----+--------------------+--------------------+----------+-------+-----+\n",
      "|  5872878|2015-03-02T18:42:...|      Lars Reeker|        0|70185|2015-03-02T18:42:...|https://lh3.googl...|         1|      0|    0|\n",
      "|  5872995|2015-03-02T19:04:...|              Vra|        0|70186|2015-03-06T15:45:...|                null|         6|      0|    1|\n",
      "|  5873177|2015-03-02T19:40:...|           Aroona|        0|70187|2015-03-02T19:40:...|https://www.grava...|         1|      0|    0|\n",
      "|  5873184|2015-03-02T19:46:...|           Yazeed|        0|70188|2015-03-02T19:46:...|https://www.grava...|         1|      0|    0|\n",
      "|   228681|2015-03-02T19:56:...|           Taimur|        0|70189|2015-03-03T09:26:...|http://i.stack.im...|       101|      0|    0|\n",
      "|  5873275|2015-03-02T19:59:...|  Chris Marciniak|        0|70190|2015-03-02T19:59:...|                null|         1|      0|    0|\n",
      "|   443263|2015-03-02T20:08:...|five_dollar_shake|        0|70191|2015-03-06T23:25:...|https://www.grava...|         1|      0|    1|\n",
      "|  3666317|2015-03-02T20:10:...|      user3054661|        0|70192|2015-03-02T20:10:...|https://www.grava...|         1|      0|    0|\n",
      "|  4607230|2015-03-02T20:41:...|   Steve Zelaznik|        0|70193|2015-03-06T19:21:...|http://i.stack.im...|         1|      0|    1|\n",
      "|  1973876|2015-03-02T20:46:...|         RGHummel|        0|70194|2015-03-07T18:46:...|https://www.grava...|        11|      0|    1|\n",
      "|  5873487|2015-03-02T20:52:...|          pedwink|        0|70195|2015-03-03T17:54:...|                null|         1|      0|    0|\n",
      "|  5873514|2015-03-02T20:57:...|         Bernard |        0|70196|2015-03-05T20:44:...|https://www.grava...|         6|      0|    0|\n",
      "|  5873574|2015-03-02T21:08:...|             kaym|        0|70197|2015-03-05T00:46:...|                null|        18|      1|    1|\n",
      "|  5873703|2015-03-02T21:37:...| daniele franzosi|        0|70198|2015-03-02T22:10:...|https://www.grava...|         1|      0|    0|\n",
      "|  5873708|2015-03-02T21:38:...|              Ray|        0|70199|2015-03-04T13:09:...|https://www.grava...|        51|      0|    4|\n",
      "|  5873938|2015-03-02T22:35:...|             Chad|        0|70200|2015-03-06T19:51:...|https://www.grava...|        11|      0|    0|\n",
      "|  5873960|2015-03-02T22:39:...|           Daniel|        0|70201|2015-03-06T11:25:...|                null|         1|      0|    0|\n",
      "|   136272|2015-03-02T23:33:...|          gvrocha|        0|70202|2015-03-02T23:33:...|https://www.grava...|       101|      1|    0|\n",
      "|  1959992|2015-03-02T23:41:...|   Warren Whipple|        0|70203|2015-03-07T17:52:...|https://www.grava...|       116|      0|    0|\n",
      "|  4139064|2015-03-02T23:45:...|             MYjx|        0|70204|2015-03-04T00:17:...|https://www.grava...|       103|      0|    0|\n",
      "+---------+--------------------+-----------------+---------+-----+--------------------+--------------------+----------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AccountId: string (nullable = true)\n",
      " |-- CreationDate: string (nullable = true)\n",
      " |-- DisplayName: string (nullable = true)\n",
      " |-- DownVotes: string (nullable = true)\n",
      " |-- Id: string (nullable = true)\n",
      " |-- LastAccessDate: string (nullable = true)\n",
      " |-- ProfileImageUrl: string (nullable = true)\n",
      " |-- Reputation: string (nullable = true)\n",
      " |-- UpVotes: string (nullable = true)\n",
      " |-- Views: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AccountId: string (nullable = true)\n",
      " |-- CreationDate: string (nullable = true)\n",
      " |-- DisplayName: string (nullable = true)\n",
      " |-- DownVotes: string (nullable = true)\n",
      " |-- Id: string (nullable = true)\n",
      " |-- LastAccessDate: string (nullable = true)\n",
      " |-- ProfileImageUrl: string (nullable = true)\n",
      " |-- Reputation: string (nullable = true)\n",
      " |-- UpVotes: string (nullable = true)\n",
      " |-- Views: string (nullable = true)\n",
      " |-- CreationDate_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"CreationDate_ts\",\n",
    "        to_timestamp(\"CreationDate\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 5: Identify veterans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "It can be interesting to think about what factors influence a user to remain active on the site over a long period of time. In order not to bias the results towards older users, we'll define a time window between 100 and 150 days after account creation. If the user has made a post in this time, we'll consider them active and well on their way to being veterans of the site; if not, they are inactive and were likely brief users.\n",
    "\n",
    "*Consider*: What other parameterizations of \"activity\" could we use, and how would they differ in terms of splitting our user base?\n",
    "\n",
    "*Consider*: What other biases are still not dealt with, after using the above approach?\n",
    "\n",
    "Let's see if there are differences between the first ever question posts of \"veterans\" vs. \"brief users\". For each group separately, average the score, views, number of answers, and number of favorites of the users' **first question**. Remember, if the score, views, answers, or favorites is missing, you should assume it is zero.\n",
    "\n",
    "*Consider*: What story could you tell from these numbers? How do the numbers support it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "* Total brief users: 24,864\n",
    "* Total veteran users: 2,027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53/4134826275.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext\n",
    "# import xml.etree.ElementTree as ET   \n",
    "# import os\n",
    "# sc = SparkContext(\"local[*]\", \"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_stats_data = sc.textFile('./spark-stats-data/allPosts')\n",
    "user_stats_data = sc.textFile('./spark-stats-data/allUsers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_user_cr_date_no_rep_data(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            root = ET.fromstring(line)\n",
    "            if \"Id\" in root.attrib:\n",
    "                user_id = root.attrib['Id']\n",
    "                if 'CreationDate' in root.attrib:\n",
    "                    creation_date = datetime.strptime(\n",
    "                        root.attrib['CreationDate'], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "                else:\n",
    "                    creation_date = None\n",
    "                return (int(user_id), creation_date)\n",
    "            else:\n",
    "                return ('empty')\n",
    "\n",
    "        except:\n",
    "            return ('empty')\n",
    "    else:\n",
    "        return ('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_users = user_stats_data.map(\n",
    "    filter_user_cr_date_no_rep_data).filter(lambda x: x != 'empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_user_cre_date(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            parsed_line = ET.fromstring(line)\n",
    "            if \"OwnerUserId\" and 'CreationDate' in parsed_line.attrib:\n",
    "                user_id = parsed_line.attrib['OwnerUserId']\n",
    "                creation_date = datetime.strptime(\n",
    "                    parsed_line.attrib['CreationDate'], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "                return (int(user_id), creation_date)\n",
    "            else:\n",
    "                return ('empty')\n",
    "        except:\n",
    "            return ('empty')\n",
    "    else:\n",
    "        return ('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_posts = posts_stats_data.map(\n",
    "    post_user_cre_date).filter(lambda x: x != 'empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_delta(after, before):\n",
    "    if before is not None and after is not None:\n",
    "        diff = (after - before).days\n",
    "        return diff if diff > 0 else 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def validity_check(days):\n",
    "    if days is None:\n",
    "        return 0\n",
    "    if int(days) >= 100 and int(days) <= 150:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_cat = valid_users.join(valid_posts)\\\n",
    "    .map(lambda x: (x[0], time_delta(x[1][1], x[1][0])))\\\n",
    "    .map(lambda x: (x[0], validity_check(x[1])))\\\n",
    "    .reduceByKey(lambda x, y: max(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "veterans = all_user_cat.filter(lambda x: x[1] == 1)\n",
    "brief_views = all_user_cat.filter(lambda x: x[1] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_info_data(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            parsed_line = ET.fromstring(line)\n",
    "            if \"PostTypeId\" and \"OwnerUserId\" and 'CreationDate' in parsed_line.attrib:\n",
    "                if int(parsed_line.attrib['PostTypeId']) == 1:\n",
    "                    user_id = parsed_line.attrib['OwnerUserId']\n",
    "                    creation_date = datetime.strptime(\n",
    "                        parsed_line.attrib['CreationDate'], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "                    answer_count = parsed_line.attrib.get(\"AnswerCount\", 0)\n",
    "                    view_count = parsed_line.attrib.get(\"ViewCount\", 0)\n",
    "                    fav_score = parsed_line.attrib.get(\"FavoriteCount\", 0)\n",
    "                    score = parsed_line.attrib.get(\"Score\", 0)\n",
    "                    return (int(user_id), creation_date, int(score), int(view_count), int(answer_count), int(fav_score))\n",
    "                else:\n",
    "                    return ('empty')\n",
    "\n",
    "            else:\n",
    "                return ('empty')\n",
    "        except:\n",
    "            return ('empty')\n",
    "    else:\n",
    "        return ('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_question = posts_stats_data.map(lambda x: post_info_data(x))\\\n",
    "    .filter(lambda x: x != 'empty')\\\n",
    "    .map(lambda x: ((x[0], x[1]), (x[2], x[3], x[4], x[5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_ques = all_question.map(lambda x: (x[0][0], x[0][1]))\\\n",
    "    .reduceByKey(lambda x, y: min(x, y))\\\n",
    "    .map(lambda x: ((x[0], x[1]), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_first_q_stats = first_ques.join(all_question)\\\n",
    "    .map(lambda x: (x[0][0], (x[1][1][0], x[1][1][1], x[1][1][2], x[1][1][3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "vet_score, vet_views, vet_answers, vet_favorites, vet_count = veterans.join(all_user_first_q_stats)\\\n",
    "    .map(lambda x: (x[0], (x[1][1][0], x[1][1][1], x[1][1][2], x[1][1][3])))\\\n",
    "    .map(lambda x: (x[1][0], x[1][1], x[1][2], x[1][3], 1))\\\n",
    "    .reduce(lambda x, y: (x[0]+y[0], x[1]+y[1], x[2]+y[2], x[3]+y[3], x[4]+y[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "brief_score, brief_views, brief_answers, brief_favorites, brief_count = brief_views.join(all_user_first_q_stats)\\\n",
    "    .map(lambda x: (x[0], (x[1][1][0], x[1][1][1], x[1][1][2], x[1][1][3])))\\\n",
    "    .map(lambda x: (x[1][0], x[1][1], x[1][2], x[1][3], 1))\\\n",
    "    .reduce(lambda x, y: (x[0]+y[0], x[1]+y[1], x[2]+y[2], x[3]+y[3], x[4]+y[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "vet_avg = [x/vet_count for x in [vet_score, vet_views, vet_answers, vet_favorites]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "brief_avg = [x/brief_count for x in [brief_score, brief_views, brief_answers, brief_favorites]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avg = vet_avg + brief_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[\"vet_score\",\"vet_views\",\"vet_answers\",\"vet_favorites\",\"brief_score\",\"brief_views\",\"brief_answers\",\"brief_favorites\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_veterans = {name:avg for name,avg in zip(names,all_avg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vet_score': 3.5322843190450355,\n",
       " 'vet_views': 927.7042864894195,\n",
       " 'vet_answers': 1.2962561041779708,\n",
       " 'vet_favorites': 1.2930005425935973,\n",
       " 'brief_score': 2.1001740930692137,\n",
       " 'brief_views': 552.9433491742342,\n",
       " 'brief_answers': 0.9704512304145297,\n",
       " 'brief_favorites': 0.5756834329271162}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identify_veterans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 6: Identify veterans&mdash;full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Same as above, but on the full Stack Exchange data set.\n",
    "\n",
    "No pre-parsed data is available for this question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "* Total brief users: 1,848,628\n",
    "* Total veteran users: 288,285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_stats_data_long = sc.textFile('./spark-stack-data/allPosts')\n",
    "user_stats_data_long = sc.textFile('./spark-stack-data/allUsers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_users = user_stats_data_long.map(\n",
    "    filter_user_cr_date_no_rep_data).filter(lambda x: x != 'empty')\n",
    "\n",
    "valid_posts = posts_stats_data_long.map(\n",
    "    post_user_cre_date).filter(lambda x: x != 'empty')\n",
    "\n",
    "all_user_cat = valid_users.join(valid_posts)\\\n",
    "    .map(lambda x: (x[0], time_delta(x[1][1], x[1][0])))\\\n",
    "    .map(lambda x: (x[0], validity_check(x[1])))\\\n",
    "    .reduceByKey(lambda x, y: max(x, y))\n",
    "\n",
    "veterans = all_user_cat.filter(lambda x: x[1] == 1)\n",
    "brief = all_user_cat.filter(lambda x: x[1] == 0)\n",
    "\n",
    "all_question = posts_stats_data_long.map(lambda x: post_info_data(x))\\\n",
    "    .filter(lambda x: x != 'empty')\\\n",
    "    .map(lambda x: ((x[0], x[1]), (x[2], x[3], x[4], x[5])))\n",
    "\n",
    "first_ques = all_question.map(lambda x: (x[0][0], x[0][1]))\\\n",
    "    .reduceByKey(lambda x, y: min(x, y))\\\n",
    "    .map(lambda x: ((x[0], x[1]), 1))\n",
    "\n",
    "all_user_first_q_stats = first_ques.join(all_question)\\\n",
    "    .map(lambda x: (x[0][0], (x[1][1][0], x[1][1][1], x[1][1][2], x[1][1][3])))\n",
    "\n",
    "vet_score, vet_views, vet_answers, vet_favorites, vet_count = veterans.join(all_user_first_q_stats)\\\n",
    "    .map(lambda x: (x[0], (x[1][1][0], x[1][1][1], x[1][1][2], x[1][1][3])))\\\n",
    "    .map(lambda x: (x[1][0], x[1][1], x[1][2], x[1][3], 1))\\\n",
    "    .reduce(lambda x, y: (x[0]+y[0], x[1]+y[1], x[2]+y[2], x[3]+y[3], x[4]+y[4]))\n",
    "\n",
    "brief_score, brief_views, brief_answers, brief_favorites, brief_count = brief.join(all_user_first_q_stats)\\\n",
    "    .map(lambda x: (x[0], (x[1][1][0], x[1][1][1], x[1][1][2], x[1][1][3])))\\\n",
    "    .map(lambda x: (x[1][0], x[1][1], x[1][2], x[1][3], 1))\\\n",
    "    .reduce(lambda x, y: (x[0]+y[0], x[1]+y[1], x[2]+y[2], x[3]+y[3], x[4]+y[4]))\n",
    "\n",
    "vet_avg = [x/vet_count for x in [vet_score, vet_views, vet_answers, vet_favorites]]\n",
    "\n",
    "brief_avg = [x/brief_count for x in [brief_score, brief_views, brief_answers, brief_favorites]]\n",
    "\n",
    "all_avg = vet_avg + brief_avg\n",
    "\n",
    "names=[\"vet_score\",\"vet_views\",\"vet_answers\",\"vet_favorites\",\"brief_score\",\"brief_views\",\"brief_answers\",\"brief_favorites\"]\n",
    "\n",
    "identify_veterans_full = {name:avg for name,avg in zip(names,all_avg)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 7: Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Word2Vec is an alternative approach for vectorizing text data. The vectorized representations of words in the vocabulary tend to be useful for predicting other words in the document, hence the famous example \"vector('king') - vector('man') + vector('woman') ~= vector('queen')\".\n",
    "\n",
    "Let's see how good a Word2Vec model we can train using the **tags** of each Stack Exchange post as documents (this uses the full data set). Use the implementation of Word2Vec from Spark ML (this will require using DataFrames) to return a list of the top 25 closest synonyms to \"ggplot2\" and their similarity score in tuple format (\"string\", number).\n",
    "\n",
    "The tags appear in the data as one string, you will need to separate them into individual tags. There is no need to further parse them beyond separating them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The dimensionality of the vector space should be 100. The random seed should be 42 in `PySpark`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "* Mean of the top 25 cosine similarities: 0.8012362027168274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5079/4134826275.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import re\n",
    "from lxml import etree\n",
    "from lxml.etree import XMLSyntaxError\n",
    "\n",
    "sc = SparkContext(\"local[*]\", \"temp\")\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posts = sc.textFile('./spark-stack-data/allPosts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_post(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            root = etree.fromstring(line)\n",
    "            if \"Tags\" in root.attrib:\n",
    "                tag = root.attrib['Tags']\n",
    "                return tag\n",
    "            else:\n",
    "                return ('empty')\n",
    "        except XMLSyntaxError:\n",
    "            return ('empty')\n",
    "    else:\n",
    "        return ('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_posts = posts.map(tag_post).filter(lambda x: x!= 'empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<c#><html><asp.net-mvc><validation><razor>',\n",
       " '<java><ftp><ftp-client>',\n",
       " '<sharepoint><web-parts>',\n",
       " '<ios><animation><frameworks><static><crash>',\n",
       " '<android><android-activity><organization>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_posts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|score|\n",
      "+--------------------+-----+\n",
      "|[c#, html, asp.ne...|    1|\n",
      "|[java, ftp, ftp-c...|    1|\n",
      "|[sharepoint, web-...|    1|\n",
      "|[ios, animation, ...|    1|\n",
      "|[android, android...|    1|\n",
      "|        [freeswitch]|    1|\n",
      "|[mysql, command-p...|    1|\n",
      "|   [sql, sql-server]|    1|\n",
      "|   [android, aptana]|    1|\n",
      "|        [php, mysql]|    1|\n",
      "|[iphone, iphone-p...|    1|\n",
      "|[c#, jquery, json...|    1|\n",
      "|      [php, mongodb]|    1|\n",
      "|[excel, vba, init...|    1|\n",
      "|[cryptography, pa...|    1|\n",
      "|[url, jboss, loca...|    1|\n",
      "|[hibernate, jquer...|    1|\n",
      "|[jquery, jqgrid, ...|    1|\n",
      "|[assembly, strcat...|    1|\n",
      "|  [linux, vim, sudo]|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tag_posts.map(lambda line: ([s for s in re.split(\"<|>\", line) if s != ''], 1))\\\n",
    "            .toDF(['text', 'score'])\n",
    "\n",
    "w2v = Word2Vec(inputCol=\"text\", outputCol=\"vectors\", vectorSize=100,minCount=10,seed=42)\n",
    "model = w2v.fit(df)\n",
    "result = model.transform(df)\n",
    "\n",
    "# print(model.findSynonyms('ggplot2', 25).rdd.take(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = model.findSynonyms('ggplot2', 25).rdd.take(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lattice', 0.8926142454147339),\n",
       " ('r-grid', 0.8519014120101929),\n",
       " ('plotrix', 0.8382738828659058),\n",
       " ('line-plot', 0.8260414004325867),\n",
       " ('ecdf', 0.815007209777832),\n",
       " ('levelplot', 0.8147303462028503),\n",
       " ('ggvis', 0.8136793375015259),\n",
       " ('quantile', 0.8123936057090759),\n",
       " ('boxplot', 0.808078944683075),\n",
       " ('standard-error', 0.8051626086235046),\n",
       " ('loess', 0.799207329750061),\n",
       " ('rgl', 0.7975688576698303),\n",
       " ('plotmath', 0.7954445481300354),\n",
       " ('categorical-data', 0.7846398949623108),\n",
       " ('density-plot', 0.7797976732254028),\n",
       " ('do.call', 0.7794148325920105),\n",
       " ('gridextra', 0.7787784934043884),\n",
       " ('r-factor', 0.7784388661384583),\n",
       " ('r-raster', 0.778320848941803),\n",
       " ('weibull', 0.7760669589042664),\n",
       " ('tapply', 0.7753050923347473),\n",
       " ('lm', 0.7751830816268921),\n",
       " ('vegan', 0.7751252055168152),\n",
       " ('dplyr', 0.7744709849357605),\n",
       " ('mgcv', 0.7738687992095947)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec = [(x['word'],x['similarity']) for x in word_list]\n",
    "word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 8: Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We'd like to see if we can predict the tags of a **question** from its body text. Instead of predicting specific tags, we will instead try to predict if a question contains one of the top ten most common tags.  \n",
    "\n",
    "To this end, we have separated out a train and a test set from the original data.  The training and tests sets were downloaded with the stats data at the beginning of the notebook.  You can also get them from S3:\n",
    "  * `s3://dataincubator-course/spark-stats-data/posts_train.zip`\n",
    "  * `s3://dataincubator-course/spark-stats-data/posts_test.zip`\n",
    "\n",
    "This will involve two steps: first, find the ten most common tags for questions in the training data set (the tags have been removed from the test set). Then train a learner to predict from the text of the question (the `Body` attribute) if it should have one of those ten tags in it - you will need to process the question text with NLP techniques such as splitting the text into tokens.\n",
    "\n",
    "Since we can't reliably pickle Spark models, instead return a list of your predictions, sorted by the question's `Id`.  This sorting is very important, as our grader expects the results to be submitted in a particular order. These predictions should be `0` if the question isn't expected to have a tag in the top ten, and `1` if it is.\n",
    "\n",
    "As an example, if our top tags include `spark` and `python`, and we had the following questions:\n",
    "\n",
    "```\n",
    "<row Body=\"...\" Id=\"1740\" Tags=\"<machine-learning><spark><regression>\" ... />\n",
    "<row Body=\"...\" Id=\"723\" Tags=\"<statistics><neurons>\" ... />\n",
    "<row Body=\"...\" Id=\"2740\" Tags=\"<functional><python><spark><pyspark>\" ... />\n",
    "```\n",
    "\n",
    "We would expect to return `[0, 1, 1]` (for the order `[723, 1740, 2740]`).  You may need to do some format manipulation in your DataFrame to get this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "- Number of training posts with a tag in the top 10: `22525`\n",
    "- Number without: `19540`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  spark-stats-data/posts_train.zip\n",
      "  inflating: spark-stats-data/train/part-00001  \n",
      "  inflating: spark-stats-data/train/part-00002  \n",
      "  inflating: spark-stats-data/train/part-00003  \n",
      "  inflating: spark-stats-data/train/part-00004  \n",
      "  inflating: spark-stats-data/train/part-00005  \n",
      "  inflating: spark-stats-data/train/part-00006  \n",
      "  inflating: spark-stats-data/train/part-00007  \n",
      "  inflating: spark-stats-data/train/part-00008  \n",
      "  inflating: spark-stats-data/train/part-00009  \n",
      "  inflating: spark-stats-data/train/part-00010  \n",
      "Archive:  spark-stats-data/posts_test.zip\n",
      "  inflating: spark-stats-data/test/part-00001  \n",
      "  inflating: spark-stats-data/test/part-00002  \n",
      "  inflating: spark-stats-data/test/part-00003  \n",
      "  inflating: spark-stats-data/test/part-00004  \n",
      "  inflating: spark-stats-data/test/part-00005  \n",
      "  inflating: spark-stats-data/test/part-00006  \n",
      "  inflating: spark-stats-data/test/part-00007  \n",
      "  inflating: spark-stats-data/test/part-00008  \n",
      "  inflating: spark-stats-data/test/part-00009  \n",
      "  inflating: spark-stats-data/test/part-00010  \n"
     ]
    }
   ],
   "source": [
    "# !unzip -d spark-stats-data/train spark-stats-data/posts_train.zip\n",
    "# !unzip -d spark-stats-data/test spark-stats-data/posts_test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import re\n",
    "from lxml import etree\n",
    "from lxml.etree import XMLSyntaxError\n",
    "\n",
    "sc = SparkContext(\"local[*]\", \"temp\")\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sc.textFile('./spark-stats-data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  <row AnswerCount=\"0\" Body=\"&lt;p&gt;I\\'m developing an application in which users can create \\'sections\\' (à la subreddit in reddit), in which items/posts can be created and voted with a thumbs-up/down system.&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.evanmiller.org/how-not-to-sort-by-average-rating.html&quot; rel=&quot;nofollow&quot;&gt;A great article&lt;/a&gt; guided me on how to sort these votes so that an item with a 100% positive response but with few votes won\\'t get ranked over one with hundreds of votes and an acceptance of 80%. The article describes it pretty well.&lt;/p&gt;&#10;&#10;&lt;p&gt;However, I\\'d like to discard the lowest-ranked items and this is where it gets tricky:&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;How could I know the minimum number of votes in order to discard it?&lt;/li&gt;&#10;&lt;li&gt;What is the score\\'s threshold required to discard the item?&lt;/li&gt;&#10;&lt;/ul&gt;&#10;&#10;&lt;p&gt;As I said, there are sections, and each one has items (which are the ones voted). The formula has to take into consideration the fact that one section may have 100 items with thousands of votes and another might have 3 or 4 items with 20 votes, so a minimum of 40 votes required might be optimal for the first case but totally out of bounds for the second.&lt;/p&gt;&#10;&#10;&lt;p&gt;(I was tempted in posting this to MathOverflow, but I\\'m not really sure since this also involves some programming)&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks!&lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2011-05-16T17:39:58.170\" FavoriteCount=\"0\" Id=\"10893\" LastActivityDate=\"2011-06-02T22:10:16.550\" LastEditDate=\"2011-06-02T22:10:16.550\" LastEditorUserId=\"88\" OwnerDisplayName=\"metrobalderas\" OwnerUserId=\"4638\" PostTypeId=\"1\" Score=\"2\" Tags=\"&lt;confidence-interval&gt;\" Title=\"Formula to discard items by votes (Lower bound of Wilson score confidence interval)\" ViewCount=\"223\" />'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.take(2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tag_string(tags_string):\n",
    "    tags = [s for s in re.split(\"<|>\", tags_string) if s != '']\n",
    "    tags_count = [(t, 1) for t in tags]\n",
    "    return tags_count\n",
    "\n",
    "\n",
    "def retrieve_tags(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            root = etree.fromstring(line)\n",
    "            if 'Id' and 'Body' and \"Tags\" in root.attrib:\n",
    "                tags_string = root.attrib['Tags']\n",
    "                return (split_tag_string(tags_string))\n",
    "            else:\n",
    "                return ('empty')\n",
    "        except XMLSyntaxError:\n",
    "            return ('empty')\n",
    "    else:\n",
    "        return ('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r',\n",
       " 'regression',\n",
       " 'time-series',\n",
       " 'machine-learning',\n",
       " 'probability',\n",
       " 'hypothesis-testing',\n",
       " 'distributions',\n",
       " 'self-study',\n",
       " 'logistic',\n",
       " 'correlation']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_10_tags_with_count = train.map(retrieve_tags)\\\n",
    "    .filter(lambda x: x != 'empty')\\\n",
    "    .flatMap(lambda x: x)\\\n",
    "    .reduceByKey(lambda x, y: x+y)\\\n",
    "    .takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "common_10_tags = [x[0] for x in common_10_tags_with_count]\n",
    "common_10_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_check(tags_string):\n",
    "    tags= [s for s in re.split(\"<|>\", tags_string) if s != '']\n",
    "    for c_tag in common_10_tags:\n",
    "        if c_tag in tags:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_body_tag_class(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            root = etree.fromstring(line)\n",
    "            if 'Id' and 'Body' and \"Tags\" in root.attrib:\n",
    "                _id = root.attrib['Id']\n",
    "                body =  ' '.join([s for s in re.split(\"\\n\", BeautifulSoup(root.attrib['Body']).get_text())])\n",
    "                tags_string = root.attrib['Tags']\n",
    "                return (body,tag_check(tags_string))\n",
    "            else:\n",
    "                return ('empty')\n",
    "        except XMLSyntaxError:\n",
    "            return ('empty')\n",
    "    else:\n",
    "        return ('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.map(retrieve_body_tag_class).filter(lambda x: x != 'empty').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19540\n",
      "22525\n"
     ]
    }
   ],
   "source": [
    "##checkpoint\n",
    "print(train.map(retrieve_body_tag_class).filter(lambda x: x != 'Empty').filter(lambda x: x[1]==0).count())\n",
    "print(train.map(retrieve_body_tag_class).filter(lambda x: x != 'Empty').filter(lambda x: x[1]==1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                body|label|\n",
      "+--------------------+-----+\n",
      "|I'm developing an...|    0|\n",
      "|I would like to c...|    1|\n",
      "|I have to generat...|    1|\n",
      "|I have searched f...|    1|\n",
      "|I have data with ...|    1|\n",
      "|I have a 37-quest...|    0|\n",
      "|I'm trying to do ...|    1|\n",
      "|I am looking for ...|    0|\n",
      "|As question, I ha...|    1|\n",
      "|If i test two hyp...|    1|\n",
      "|I have a mixed ef...|    0|\n",
      "|If an exploratory...|    1|\n",
      "|I am trying to fi...|    1|\n",
      "|I want to estimat...|    1|\n",
      "|I have a question...|    0|\n",
      "|I have a data set...|    0|\n",
      "|I am trying to in...|    0|\n",
      "|I am trying to mo...|    1|\n",
      "|I have data like ...|    0|\n",
      "|I'm using the ets...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##save the computation from the traing process by cache()\n",
    "training_df = sqlContext.createDataFrame(train_data, [\"body\", \"label\"]).cache()\n",
    "training_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range(50, 2000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.logspace(0,10,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer,RegexTokenizer\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "regex_tokenizer = RegexTokenizer(inputCol=\"body\", outputCol=\"words\", pattern=\"\\\\w\")\n",
    "tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"features\")\n",
    "logreg = LogisticRegression(maxIter=1000000, regParam=0.8)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer,remover, hashingTF, logreg])\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "paramGrid = (ParamGridBuilder() \n",
    "    .addGrid(hashingTF.numFeatures, [2000])\n",
    "    .addGrid(logreg.regParam, [10, 1, 0.1]) \n",
    "    .build())\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(training_df)\n",
    "best_model = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sc.textFile('./spark-stats-data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_body_id(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            root = etree.fromstring(line)\n",
    "            if 'Id' and 'Body' and \"PostTypeId\" in root.attrib:\n",
    "                if int(root.attrib['PostTypeId']) == 1:\n",
    "                    _id = root.attrib['Id']\n",
    "                    body = ' '.join([s for s in re.split(\"\\n\", BeautifulSoup(root.attrib['Body']).get_text())])\n",
    "                    tags_string = root.attrib['Tags']\n",
    "                    return (body,int(_id))\n",
    "                else:\n",
    "                    return ('empty')\n",
    "            else:\n",
    "                return ('empty')\n",
    "        except XMLSyntaxError:\n",
    "            return ('empty')\n",
    "    else:\n",
    "        return ('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.map(retrieve_body_id).filter(lambda x: x != 'empty').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = sqlContext.createDataFrame(test_data, [\"body\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_prediction = cvModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "better_prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected = better_prediction.select(\"id\", \"prediction\").sort('id').collect()\n",
    "classification = [int(x['prediction']) for x in selected]\n",
    "len(classification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbclean": true,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
